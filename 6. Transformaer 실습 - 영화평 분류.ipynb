{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718e3bb8-527a-46c6-9fc4-0328a064bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 예제는 `2. RNN 실습 - 영화평 분류`\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import SimpleRNN, Dense, Input, Embedding\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(80,))) # 입력하는 영화평의 길이를 80으로 제한, 길면 자르고, 짧으면 zero padding\n",
    "# model.add(Embedding(input_dim=10000, output_dim=32))\n",
    "# model.add(SimpleRNN(64))\n",
    "# model.add(Dense(2, activation='softmax')) # model.add(Dense(1, activation='sigmoid'))\n",
    "# model.summary()\n",
    "\n",
    "# 이 모델의 test set accuracy는 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afd00e5-8b06-45ae-aff3-7eb8fbd42e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # 헤드의 수\n",
    "        self.key_dim = key_dim # 각 헤드의 차원\n",
    "        self.depth = key_dim // num_heads # 각 헤드의 깊이, shape 변경 \n",
    "\n",
    "        self.wq = Dense(key_dim) # q = self.wq(q)\n",
    "        self.wk = Dense(key_dim)\n",
    "        self.wv = Dense(key_dim)\n",
    "\n",
    "        self.dense = Dense(key_dim) # 출력 레이어\n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 텐서의 모양 batch_size, seq_len, key_dim -> batch_size, num_heads, seq_len, depth\n",
    "        x = tf.reshape(x, (batch_size, self.num_heads, -1, self.depth))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # 모델을 만들어 반환\n",
    "    # MultiHeadAttention(3, 32)(v, k, q) <- v, k, q는 모두 pos_enc_output이 전달\n",
    "    def call(self, q, k, v): \n",
    "        # 입력 텐서가 weight와 결합되어서 q, k, v로 변환\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        # ndarray.shape -> 2,3,4\n",
    "        # tf.shape(텐서) -> 2,3,4 -> 2가 배치크기, shape 중에서 0번째꺼\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # 각 헤드로 분할\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # scaled dot product 계산\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) # Q @ K.T\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        scaled_attention = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        # 각 헤드의 출력을 원래의 형태로 변환\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c7484a-7c63-419b-bc71-6e5a7520082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 80)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 80, 32)               320000    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 80, 32)               0         ['embedding[0][0]']           \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, None, 32)             4224      ['tf.__operators__.add[0][0]',\n",
      " iHeadAttention)                                                     'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " add (Add)                   (None, 80, 32)               0         ['tf.__operators__.add[0][0]',\n",
      "                                                                     'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 80, 32)               128       ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 80, 32)               4192      ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 80, 32)               0         ['sequential[0][0]',          \n",
      "                                                                     'batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 80, 32)               128       ['add_1[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 32)                   0         ['batch_normalization_1[0][0]'\n",
      " GlobalAveragePooling1D)                                            ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 32)                   0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 64)                   2112      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 64)                   0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 2)                    130       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 330914 (1.26 MB)\n",
      "Trainable params: 330786 (1.26 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transformer를 이용한 영화평 분류\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "inputs = layers.Input(shape=(80,))\n",
    "\n",
    "input_embedding = layers.Embedding(input_dim=10000, output_dim=32)(inputs)\n",
    "positions = tf.range(start=0, limit=80)\n",
    "pos_encoding = layers.Embedding(input_dim=80, output_dim=32)(positions)\n",
    "pos_enc_output = pos_encoding + input_embedding\n",
    "\n",
    "# attention_output = layers.MultiHeadAttention(num_heads=3, key_dim=32)(pos_enc_output, pos_enc_output)\n",
    "# 직접 구현한 어텐션은 헤드의 수를 2 또는 4개로 하세요.\n",
    "attention_output = MultiHeadAttention(num_heads=2, key_dim=32)(pos_enc_output, pos_enc_output, pos_enc_output)\n",
    "x = layers.add([pos_enc_output, attention_output])\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "ffnn = Sequential([layers.Dense(64, activation=\"relu\"), \n",
    "                   layers.Dense(32, activation=\"relu\")])(x)\n",
    "x = layers.add([ffnn, x])\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d441c95-8cfa-424c-bff7-0ca07b5ee8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb67f2f-7b73-4513-a2aa-7ddb5fd27cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5522cb80-c068-4b89-9152-db2a84310941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(X_train, maxlen=80, truncating='post', padding='post')\n",
    "X_test_pad = pad_sequences(X_test, maxlen=80, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf49cac-73fd-4f6c-bf10-7410080c3fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 7s 45ms/step - loss: 0.4843 - accuracy: 0.7609\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 6s 45ms/step - loss: 0.3193 - accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 5s 43ms/step - loss: 0.2659 - accuracy: 0.8911\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 5s 43ms/step - loss: 0.2189 - accuracy: 0.9064\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 5s 44ms/step - loss: 0.1744 - accuracy: 0.9228\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 6s 45ms/step - loss: 0.1369 - accuracy: 0.9380\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 6s 47ms/step - loss: 0.1047 - accuracy: 0.9579\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 5s 42ms/step - loss: 0.0862 - accuracy: 0.9662\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 5s 43ms/step - loss: 0.0657 - accuracy: 0.9770\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 5s 41ms/step - loss: 0.0468 - accuracy: 0.9842\n",
      "CPU times: total: 26.7 s\n",
      "Wall time: 55.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24c8e4aa610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9592949-9039-4f56-b4c7-aaf10dda66b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 1.6363 - accuracy: 0.7589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6363104581832886, 0.7589200139045715]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9871fb65-5168-4700-b0fc-ceea109d6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e78be4ad-f3bc-4491-aa19-19ea405b5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred = model.predict(X_test_pad)\n",
    "# pred = (pred > 0.5).astype(int)\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d81e2f-79b0-4951-9cb9-7ff0ec51ed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8648  3852]\n",
      " [ 2175 10325]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50b5985-3cad-4ea8-a6d3-b249ee5bc058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe57650-9039-4065-92d1-6e9ea3bb4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저를 sgd로 바꿔보세요. accuracy: 0.57784\n",
    "# 전체 단어의 개수를 1000개로 바꿔보세요. accuracy:\n",
    "# 영화평의 길이를 200개로 바꿔보세요. accuracy:\n",
    "# pad_sequence의 truncating과 padding을 pre로 바꿔보세요. accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f95327-4c20-401d-b00b-39427aa05552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 택스트를 긍/부정 분류하세요.\n",
    "text = \"My God the actors who potrayed the VIP people cannot act. I cringed everytime they said a line. It felt like they were just reading them. Even the intonation was off. It was like when we were kids and had to read a play in class and we exagerated the intonation. Terrible, just awful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b2c664-aa01-4d76-aab1-8f9bbefe3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "# print(word_index)\n",
    "# {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "253ed3c5-c674-45f2-980b-08c305a562a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {k:(v+3) for k,v in word_index.items()}\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "word_to_idx[\"<START>\"] = 1\n",
    "word_to_idx[\"<UNK>\"] = 2  # unknown\n",
    "word_to_idx[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e89c7e2-397f-48e8-bd1f-31952b42d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'god', 'the', 'actors', 'who', 'potrayed', 'the', 'vip', 'people', 'cannot', 'act.', 'i', 'cringed', 'everytime', 'they', 'said', 'a', 'line.', 'it', 'felt', 'like', 'they', 'were', 'just', 'reading', 'them.', 'even', 'the', 'intonation', 'was', 'off.', 'it', 'was', 'like', 'when', 'we', 'were', 'kids', 'and', 'had', 'to', 'read', 'a', 'play', 'in', 'class', 'and', 'we', 'exagerated', 'the', 'intonation.', 'terrible,', 'just', 'awful.']\n"
     ]
    }
   ],
   "source": [
    "input_text = text.lower().split()\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6056813d-003a-4700-853c-a5e4fbbe425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 숫자로 변환합니다. \n",
    "# word_to_idx에 없는 단어는 2(<UNK>)로 지정하며, \n",
    "# 인덱스가 10000보다 크면 3(<UNUSED>)로 지정합니다.\n",
    "def encoding(review_text):\n",
    "  encoded = []\n",
    "  for word in review_text:\n",
    "    try:\n",
    "      idx = word_to_idx[word]\n",
    "      if idx>10000:\n",
    "        encoded.append(3)\n",
    "      else:\n",
    "        encoded.append(idx)\n",
    "    except:\n",
    "      encoded.append(2)\n",
    "  return encoded\n",
    "\n",
    "input_encoded = encoding(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0bf4c9-244a-43fe-b72d-2ee076c7df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61, 558,   4, 156,  37,   2,   4,   3,  84, 566,   2,  13,   3,\n",
       "          3,  36, 301,   6,   2,  12, 421,  40,  36,  71,  43, 886,   2,\n",
       "         60,   4,   3,  16,   2,  12,  16,  40,  54,  75,  71, 362,   5,\n",
       "         69,   8, 332,   6, 297,  11, 707,   5,  75,   3,   4,   2,   2,\n",
       "         43,   2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_encoded)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feab6109-b316-4204-a003-9c2f56b37c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pad = pad_sequences(\n",
    "    np.array(input_encoded)[np.newaxis, :],\n",
    "    maxlen=80, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b8635c5-fffe-4f55-b544-0c49eb6439ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.28110713, 0.7188928 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_pad) # LSTM을 이용한 인공신경망 모형은 부정(0)으로 분류함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b567b-7c6d-4bab-be62-6abc00481d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.14",
   "language": "python",
   "name": "tf2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
