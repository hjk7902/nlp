{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e891c7-1779-4439-bc7e-8b45b5dbb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "# 입력값 토큰화, 임베딩\n",
    "# 훈련을 정의하고 학습\n",
    "# 평가\n",
    "# 예측하기 위한 데이터를 토큰화, 임베딩후 predict\n",
    "# 라벨 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2e0c72-f298-4096-9378-e0a1b9955e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Model, layers\n",
    "\n",
    "\n",
    "# # 인코더 입력 정의: 영문 단어가 입력, 각 단어는 4자 길이, 모든 문자의 수는 171개\n",
    "# enc_input = layers.Input(shape=(4, 171)) \n",
    "\n",
    "# # 인코더 LSTM 정의: 모든 타임스텝의 출력을 반환하도록 설정\n",
    "# enc_output, state_h, state_c = layers.LSTM(128, return_sequences=True, return_state=True)(enc_input)\n",
    "\n",
    "# # 디코더 LSTM 정의: 모든 시퀀스의 출력을 반환하도록 설정\n",
    "# dec_input = layers.Input(shape=(3, 171)) # 한글 단어는 2자 길이 + <Start> 토큰, 모든 문자의 수는 171개\n",
    "# dec_lstm_output, _, _ = layers.LSTM(128, return_sequences=True, return_state=True)(dec_input, initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "# class SimpleAttention(layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleAttention, self).__init__()\n",
    "\n",
    "#     def call(self, encoder_outputs, decoder_output):\n",
    "#         # encoder_outputs: (batch_size, enc_timesteps, units)\n",
    "#         # decoder_output: (batch_size, 1, units)\n",
    "#         # 어텐션 스코어 계산\n",
    "#         score = tf.matmul(decoder_output, encoder_outputs, transpose_b=True)\n",
    "#         # 어텐션 가중치 계산(어텐션 분포 계산)\n",
    "#         attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "#         # 컨텍스트 벡터 계산(어텐션 벡터 계산)\n",
    "#         context_vector = tf.matmul(attention_weights, encoder_outputs)\n",
    "#         # 연결\n",
    "#         context_vectors = []\n",
    "#         context_vectors.append(context_vector)\n",
    "#         for t in range(decoder_output.shape[1]):\n",
    "#             context_vector = layers.Lambda(lambda x: attention_layer([encoder_outputs, tf.expand_dims(x[:, t, :], 1)]))(decoder_output)\n",
    "#             context_vectors.append(context_vectors)\n",
    "\n",
    "#         # 컨텍스트 벡터를 시퀀스 형태로 결합\n",
    "#         context_vectors = layers.Lambda(lambda x: tf.concat(x, axis=1))(context_vectors)\n",
    "        \n",
    "#         # 디코더 출력과 어텐션 컨텍스트 벡터 결합\n",
    "#         decoder_combined_context = layers.Concatenate(axis=-1)([dec_lstm_output, context_vectors])\n",
    "\n",
    "#         return decoder_combined_context\n",
    "\n",
    "\n",
    "# # 어텐션 메커니즘 적용\n",
    "# decoder_combined_context = SimpleAttention()(enc_output, dec_lstm_output)\n",
    "\n",
    "# # 디코더 출력층 정의: 출력 크기는 모든 문자의 수인 171, softmax 활성화 함수를 사용\n",
    "# output = layers.TimeDistributed(layers.Dense(171, activation='softmax'))(decoder_combined_context)\n",
    "\n",
    "# # 모델 정의: 인코더 입력(enc_input)과 디코더 입력(dec_input)을 모델의 입력으로, 디코더 출력을 모델의 출력으로 설정\n",
    "# model = Model(inputs=[enc_input, dec_input], outputs=[output])\n",
    "\n",
    "# # 모델 요약 출력\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98dafe81-6636-4f84-bd83-13405c32c967",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 1, 128), dtype=tf.float32, name=None), name='simple_attention/MatMul_1:0', description=\"created by layer 'simple_attention'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m context_vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dec_lstm_output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 36\u001b[0m     context_vector \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_lstm_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     context_vectors\u001b[38;5;241m.\u001b[39mappend(context_vector)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 컨텍스트 벡터를 시퀀스 형태로 결합\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2.14\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2.14\\Lib\\site-packages\\tensorflow\\python\\framework\\type_spec.py:995\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    992\u001b[0m   logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[0;32m    993\u001b[0m       \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a TypeSpec for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 1, 128), dtype=tf.float32, name=None), name='simple_attention/MatMul_1:0', description=\"created by layer 'simple_attention'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "class SimpleAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_outputs, decoder_output = inputs\n",
    "        # encoder_outputs: (batch_size, enc_timesteps, units)\n",
    "        # decoder_output: (batch_size, 1, units)\n",
    "        # 어텐션 스코어 계산 (batch_size, 1, enc_timesteps)\n",
    "        score = tf.matmul(decoder_output, encoder_outputs, transpose_b=True)\n",
    "        # 어텐션 가중치 계산 (batch_size, 1, enc_timesteps)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        # 컨텍스트 벡터 계산 (batch_size, 1, units)\n",
    "        context_vector = tf.matmul(attention_weights, encoder_outputs)\n",
    "        return context_vector\n",
    "\n",
    "# 인코더 입력 정의: 영문 단어가 입력, 각 단어는 4자 길이, 모든 문자의 수는 171개\n",
    "enc_input = layers.Input(shape=(4, 171)) \n",
    "\n",
    "# 인코더 LSTM 정의: 모든 타임스텝의 출력을 반환하도록 설정\n",
    "enc_output, state_h, state_c = layers.LSTM(128, return_sequences=True, return_state=True)(enc_input)\n",
    "\n",
    "# 디코더 LSTM 정의: 모든 시퀀스의 출력을 반환하도록 설정\n",
    "dec_input = layers.Input(shape=(3, 171)) # 한글 단어는 2자 길이 + <Start> 토큰, 모든 문자의 수는 171개\n",
    "dec_lstm_output, _, _ = layers.LSTM(128, return_sequences=True, return_state=True)(dec_input, initial_state=[state_h, state_c])\n",
    "\n",
    "# 어텐션 메커니즘 적용\n",
    "attention_layer = SimpleAttention()\n",
    "\n",
    "# 각 디코더 타임스텝에 대해 어텐션 적용\n",
    "context_vectors = []\n",
    "for t in range(dec_lstm_output.shape[1]):\n",
    "    context_vector = layers.Lambda(lambda x: attention_layer([enc_output, tf.expand_dims(x[:, t, :], 1)]))(dec_lstm_output)\n",
    "    context_vectors.append(context_vector)\n",
    "\n",
    "# 컨텍스트 벡터를 시퀀스 형태로 결합\n",
    "context_vectors = layers.Lambda(lambda x: tf.concat(x, axis=1))(context_vectors)\n",
    "\n",
    "# 디코더 출력과 어텐션 컨텍스트 벡터 결합\n",
    "decoder_combined_context = layers.Concatenate(axis=-1)([dec_lstm_output, context_vectors])\n",
    "\n",
    "# 디코더 출력층 정의: 출력 크기는 모든 문자의 수인 171, softmax 활성화 함수를 사용\n",
    "output = layers.TimeDistributed(layers.Dense(171, activation='softmax'))(decoder_combined_context)\n",
    "\n",
    "# 모델 정의: 인코더 입력(enc_input)과 디코더 입력(dec_input)을 모델의 입력으로, 디코더 출력을 모델의 출력으로 설정\n",
    "model = Model(inputs=[enc_input, dec_input], outputs=[output])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 더미 데이터 생성\n",
    "import numpy as np\n",
    "encoder_input_data = np.random.random((100, 4, 171))\n",
    "decoder_input_data = np.random.random((100, 3, 171))\n",
    "decoder_output_data = np.random.random((100, 3, 171))\n",
    "\n",
    "# 모델 학습\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8468eedd-f7cc-4073-98a1-5f22e18ef776",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam') # metrics가 없으면 훈련시 loss만 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637c7393-a8ce-4da2-ac5d-b9660283fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "# 문자배열 생성\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# <START>, <END>, <PAD>\n",
    "arr1 = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "arr2 = pd.read_csv('korean.csv', header=None)\n",
    "# print(arr2[0].values.tolist())\n",
    "num_to_char = arr1 + arr2[0].values.tolist()\n",
    "print(len(num_to_char))\n",
    "char_to_num = {char:i for i, char in enumerate(num_to_char)}\n",
    "# print(char_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1d52e1-764a-4f7d-8265-6529ccd16508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# 학습용 단어셋 불러오기\n",
    "raw = pd.read_csv('translate.csv', header=None)\n",
    "eng_kor = raw.values.tolist()\n",
    "print(len(eng_kor)) # 학습할 전체 단어 수 110개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374aa1f5-0a1a-469a-bb53-3b94de242c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 17, 24, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 171)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어를 숫자 배열로 변환\n",
    "temp_eng = 'love'\n",
    "temp_eng_n = [char_to_num[c] for c in temp_eng]\n",
    "print(temp_eng_n)\n",
    "temp_kor = '사랑'\n",
    "np.eye(171)[temp_eng_n].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777056f9-c63f-4b73-9938-4a4a6c7a2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 원-핫 인코딩된 배열로 변환\n",
    "def encode(eng_kor):\n",
    "    enc_in = []\n",
    "    dec_in = []\n",
    "    rnn_out = [] # decoder output\n",
    "    for seq in eng_kor:\n",
    "        eng = [char_to_num[c] for c in seq[0]]\n",
    "        enc_in.append(np.eye(171)[eng])\n",
    "\n",
    "        kor = [char_to_num[c] for c in ('S'+seq[1])]\n",
    "        dec_in.append(np.eye(171)[kor])\n",
    "\n",
    "        target = [char_to_num[c] for c in (seq[1] + 'E')]\n",
    "        rnn_out.append(target)\n",
    "\n",
    "    enc_in = np.array(enc_in)\n",
    "    dec_in = np.array(dec_in)\n",
    "    rnn_out = np.array(rnn_out)\n",
    "    rnn_out = np.expand_dims(rnn_out, axis=2)\n",
    "    return enc_in, dec_in, rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2784fabd-77f4-40d7-8fd1-a7043c38d67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 61],\n",
       "        [114],\n",
       "        [  1]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [['word', '단어']]\n",
    "encode(sample)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96ac495-3dd9-4804-b5b1-27ea5e867cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc, X_dec, y_rnn = encode(eng_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e2d246-e4f4-4974-a23b-82aaa074fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Concatenate.call().\n\n\u001b[1mA `Concatenate` layer should be called on a list of inputs. Received: input_shape=[TensorShape([None, 3, 128]), (None, 3, 128)]\u001b[0m\n\nArguments received by Concatenate.call():\n  • args=(['tf.Tensor(shape=(None, 3, 128), dtype=float32)', '<KerasTensor shape=(None, 3, 128), dtype=float32, sparse=False, name=keras_tensor_31>'],)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\merging\\concatenate.py:107\u001b[0m, in \u001b[0;36mConcatenate.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_output_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m    106\u001b[0m     ):\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA `Concatenate` layer should be called on a list of inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: input_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    111\u001b[0m     input_shapes \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    112\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_shapes[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Concatenate.call().\n\n\u001b[1mA `Concatenate` layer should be called on a list of inputs. Received: input_shape=[TensorShape([None, 3, 128]), (None, 3, 128)]\u001b[0m\n\nArguments received by Concatenate.call():\n  • args=(['tf.Tensor(shape=(None, 3, 128), dtype=float32)', '<KerasTensor shape=(None, 3, 128), dtype=float32, sparse=False, name=keras_tensor_31>'],)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit([X_enc, X_dec], y_rnn, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8cb3b8-4dab-4beb-bc40-2bea2f5ce309",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Concatenate.call().\n\n\u001b[1mA `Concatenate` layer should be called on a list of inputs. Received: input_shape=[TensorShape([1, 3, 128]), (None, 3, 128)]\u001b[0m\n\nArguments received by Concatenate.call():\n  • args=(['tf.Tensor(shape=(1, 3, 128), dtype=float32)', '<KerasTensor shape=(None, 3, 128), dtype=float32, sparse=False, name=keras_tensor_39>'],)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m enc_in, dec_in, _ \u001b[38;5;241m=\u001b[39m encode([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPP\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(enc_in)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([enc_in, dec_in])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(pred.shape)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m word \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\merging\\concatenate.py:107\u001b[0m, in \u001b[0;36mConcatenate.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_output_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_shape[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m    106\u001b[0m     ):\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA `Concatenate` layer should be called on a list of inputs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: input_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    111\u001b[0m     input_shapes \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    112\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_shapes[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Concatenate.call().\n\n\u001b[1mA `Concatenate` layer should be called on a list of inputs. Received: input_shape=[TensorShape([1, 3, 128]), (None, 3, 128)]\u001b[0m\n\nArguments received by Concatenate.call():\n  • args=(['tf.Tensor(shape=(1, 3, 128), dtype=float32)', '<KerasTensor shape=(None, 3, 128), dtype=float32, sparse=False, name=keras_tensor_39>'],)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "enc_in, dec_in, _ = encode([['tall', 'PP']])\n",
    "# print(enc_in)\n",
    "pred = model.predict([enc_in, dec_in])\n",
    "# print(pred.shape)\n",
    "word = np.argmax(pred[0], axis=-1)\n",
    "# print(word)\n",
    "num_to_char[word[0]], num_to_char[word[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bc287-aee2-46ed-804d-f40bcb875ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "pick = randint(0, len(eng_kor), 5)\n",
    "choose = [ [eng_kor[i][0], 'PP'] for i in pick]\n",
    "print(choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562384f-064e-4973-99b4-88223ffda263",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in, dec_in, _ = encode(choose)\n",
    "pred = model.predict([enc_in, dec_in])\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd269b-e2e7-4a43-b217-72228278aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(choose)):\n",
    "    eng = choose[i][0]\n",
    "    word = np.argmax(pred[i], axis=-1)\n",
    "    kor = ''\n",
    "    for j in range(2):\n",
    "        kor = kor + num_to_char[word[j]]\n",
    "    print(eng, kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f506b58-80ed-4ee6-a9d6-98a553f708d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d0184-cad5-49a6-8c71-0d9b7194fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.14",
   "language": "python",
   "name": "tf2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
